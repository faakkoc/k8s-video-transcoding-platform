# API Gateway - Implementation & Code Review

**Datum:** 08.02.2026 
**Status:** Code Review abgeschlossen, noch nicht deployed

---

## √úbersicht

Das API Gateway ist der zentrale Einstiegspunkt f√ºr unsere Video Transcoding Platform. Es empf√§ngt Video-Uploads, erstellt Kubernetes Jobs f√ºr das Transcoding und stellt Endpoints f√ºr Status-Abfragen und Downloads bereit.

### Architektur-Rolle

```
User/Frontend
    ‚Üì HTTP Request
API Gateway (FastAPI)
    ‚Üì Create Kubernetes Job
Transcoding Worker (FFmpeg Job)
    ‚Üì Process Video
Object Storage (sp√§ter)
```

---

## 1. FastAPI Anwendung (main.py)

### 1.1 Was ist FastAPI?

**FastAPI** ist ein modernes Python Web-Framework mit folgenden Eigenschaften:

- **Async/Await Support**: Ideal f√ºr I/O-intensive Tasks (File-Uploads, API-Calls)
- **Automatische Validierung**: Type Hints werden zu automatischen Checks
- **Auto-Documentation**: Swagger UI unter `/docs` verf√ºgbar
- **Performance**: Vergleichbar mit Node.js und Go

**Warum FastAPI f√ºr uns?**
- Video-Uploads sind I/O-intensiv ‚Üí Async hilft
- Kubernetes API-Calls sind I/O ‚Üí Async verhindert Blocking
- Built-in Docs ‚Üí einfaches Testing
- Python ‚Üí passt zu Data Science / ML Workflows

---

### 1.2 Application Setup

```python
app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    description="API Gateway for cloud-native video transcoding platform",
    docs_url=f"{settings.api_prefix}/docs",
    redoc_url=f"{settings.api_prefix}/redoc",
    openapi_url=f"{settings.api_prefix}/openapi.json",
)
```

**Was macht das?**

1. **title, version, description**: Metadaten f√ºr API-Dokumentation
2. **docs_url="/api/v1/docs"**: Swagger UI Endpoint
    - Interaktive API-Dokumentation
    - Direktes Testen von Endpoints
    - Automatisch generiert aus Code
3. **redoc_url="/api/v1/redoc"**: Alternative Dokumentation
    - Sch√∂ner, aber weniger interaktiv
    - Gut f√ºr externe API-Nutzer

**API Versioning (`/api/v1`)**

```
/api/v1/upload    ‚Üê Version 1
/api/v2/upload    ‚Üê Sp√§ter: Version 2 mit Breaking Changes
```

**Vorteil:**
- Alte Clients k√∂nnen `/api/v1` nutzen
- Neue Features in `/api/v2` ohne alte zu brechen

---

### 1.3 CORS Middleware

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

**Was ist CORS?**

**Cross-Origin Resource Sharing** - Browser-Sicherheitsmechanismus.

**Problem ohne CORS:**
```
Browser (localhost:3000 - React Frontend)
    ‚Üì GET http://localhost:8080/api/v1/videos
    ‚Üì
API (localhost:8080)
    ‚Üì
Browser: ‚ùå BLOCKED!
Error: "CORS policy: No 'Access-Control-Allow-Origin' header"
```

**Warum blockiert?**
- Verschiedene Origins (Port 3000 vs. 8080)
- Browser-Sicherheit: Verhindert Cross-Site Scripting (XSS)

**Mit CORS Middleware:**
```python
allow_origins=["*"]  # API sagt: "Ich erlaube alle Origins"
```

API-Response bekommt Header:
```
Access-Control-Allow-Origin: *
```

Browser sieht Header ‚Üí erlaubt Request.

**Security Note:**
- `["*"]` = Alle Origins (OK f√ºr Development/Testing)
- **Production**: `allow_origins=["https://frontend.example.com"]`
- **Warum?** Verhindert dass b√∂se Websites deine API nutzen

**allow_credentials=True:**
- Erlaubt Cookies/Auth-Headers in Cross-Origin Requests
- Wichtig f√ºr Authentication sp√§ter

---

### 1.4 Root Endpoint

```python
@app.get("/")
async def root():
    return {
        "service": settings.app_name,
        "version": settings.app_version,
        "docs": f"{settings.api_prefix}/docs",
        "health": f"{settings.api_prefix}/health",
    }
```

**Decorator `@app.get("/")`:**
- Registriert Funktion als Route Handler
- `"/"` = Root path (http://localhost:8080/)
- `get` = HTTP GET Method

**async def:**
- Asynchrone Funktion
- Kann andere async-Funktionen `await`-en
- Gibt Thread frei w√§hrend Warten (I/O)

**Return:**
- Dict wird automatisch zu JSON
- FastAPI macht: `json.dumps(dict)` + Content-Type Header

**Test:**
```bash
curl http://localhost:8080/
# {
#   "service": "Video Transcoding API Gateway",
#   "version": "0.1.0",
#   "docs": "/api/v1/docs",
#   "health": "/api/v1/health"
# }
```

**Zweck:**
- Discovery: Nutzer sieht wo die Docs sind
- Health Check f√ºr Load Balancer
- Version Info

---

### 1.5 Startup Event

```python
@app.on_event("startup")
async def startup_event():
    import os
    os.makedirs(settings.upload_dir, exist_ok=True)
    os.makedirs(settings.output_dir, exist_ok=True)
    print(f"üöÄ {settings.app_name} started")
```

**Wann l√§uft das?**
- **Einmal** beim Container-Start
- **Vor** dem ersten Request

**Was macht es?**

1. **Erstellt Directories:**
   ```python
   os.makedirs("/tmp/uploads", exist_ok=True)
   os.makedirs("/tmp/outputs", exist_ok=True)
   ```
    - Container-Dateisystem ist leer beim Start
    - `exist_ok=True`: Kein Error wenn schon existiert

2. **Logging:**
    - Zeigt dass Service gestartet ist
    - Wichtig f√ºr Debugging in Kubernetes

**Warum wichtig?**
- Video-Uploads gehen nach `/tmp/uploads`
- Ordner muss existieren, sonst Error
- Alternative: Volume-Mount (kommt sp√§ter)

**Sp√§ter hier:**
- Database Connection Pool initialisieren
- Redis Connection aufbauen
- Kubernetes Client initialisieren

---

### 1.6 Shutdown Event

```python
@app.on_event("shutdown")
async def shutdown_event():
    print(f"üëã {settings.app_name} shutting down")
```

**Wann l√§uft das?**
- Beim Container-Stopp
- z.B. `kubectl delete pod` oder Rolling Update

**Warum wichtig?**
- Cleanup: Connections schlie√üen
- Graceful Shutdown: Requests zu Ende bringen
- Logs f√ºr Debugging

**Sp√§ter hier:**
- Database Connections schlie√üen
- Redis Connections schlie√üen
- Laufende Uploads abbrechen

---

## 2. Dockerfile - Multi-Stage Build

### 2.1 Warum Multi-Stage?

**Problem Single-Stage:**
```dockerfile
FROM python:3.11
RUN pip install pandas numpy scikit-learn
# ‚Üí Image: 2GB (enth√§lt Build-Tools, Compiler, etc.)
```

**L√∂sung Multi-Stage:**
```dockerfile
# Stage 1: Builder (hat Build-Tools)
FROM python:3.11 as builder
RUN pip install --user pandas

# Stage 2: Runtime (nur Laufzeit)
FROM python:3.11-slim
COPY --from=builder /root/.local /home/appuser/.local
# ‚Üí Image: 500MB (nur Binaries, keine Build-Tools)
```

**Vorteil:**
- ‚úÖ Kleinere Images (schnellerer Pull)
- ‚úÖ Weniger Angriffsfl√§che (keine Compiler in Production)
- ‚úÖ Schnelleres Deployment

---

### 2.2 Stage 1: Builder

```dockerfile
FROM python:3.11-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt
```

**Zeile f√ºr Zeile:**

1. **`FROM python:3.11-slim as builder`**
    - Basis-Image: Python 3.11 auf Debian (minimale Variante)
    - `as builder`: Name dieser Stage (f√ºr COPY --from sp√§ter)

2. **`WORKDIR /app`**
    - Setzt Arbeitsverzeichnis im Container
    - Alle weiteren Commands laufen in `/app`

3. **`COPY requirements.txt .`**
    - Kopiert `requirements.txt` vom Host ‚Üí `/app/requirements.txt` im Container
    - `.` = Aktuelles Verzeichnis (`/app`)

4. **`RUN pip install --no-cache-dir --user -r requirements.txt`**
    - `--no-cache-dir`: L√∂scht Download-Cache ‚Üí spart Speicher
    - `--user`: Installiert in `/root/.local` statt System-wide
    - `-r requirements.txt`: Installiert alle Packages aus Datei

**Warum erst requirements.txt, dann Code?**

Docker cached Layers:
```
COPY requirements.txt ‚Üí Layer 1 (cached wenn requirements.txt unver√§ndert)
RUN pip install      ‚Üí Layer 2 (cached wenn Layer 1 cached)
COPY app/            ‚Üí Layer 3 (neu bei Code-√Ñnderung)
```

**Wenn du Code √§nderst:**
- Layer 3 wird neu gebaut
- Layer 1+2 bleiben gecached
- **Schnelleres Rebuild!**

---

### 2.3 Stage 2: Runtime

```dockerfile
FROM python:3.11-slim

RUN useradd -m -u 1000 appuser
WORKDIR /app
```

**Frisches Image:**
- Startet von `python:3.11-slim` (ohne Builder-Layer)
- Nur minimal n√∂tig f√ºr Runtime

**Security: Non-Root User**

```dockerfile
RUN useradd -m -u 1000 appuser
```

- `useradd`: Linux-Befehl zum User erstellen
- `-m`: Home-Directory erstellen (`/home/appuser`)
- `-u 1000`: User-ID (Standard non-root)
- `appuser`: Username

**Warum nicht root?**

**Ohne:**
```
Container l√§uft als root (UID 0)
    ‚Üì Container kompromittiert
    ‚Üì Angreifer hat root-Rechte
    ‚ùå Kann Host-System angreifen
```

**Mit non-root:**
```
Container l√§uft als appuser (UID 1000)
    ‚Üì Container kompromittiert
    ‚Üì Angreifer hat nur User-Rechte
    ‚úÖ Kann Host-System NICHT angreifen
```

**Best Practice:** Container = Least Privilege

---

### 2.4 Dependencies kopieren

```dockerfile
COPY --from=builder --chown=appuser:appuser /root/.local /home/appuser/.local
```

**Was macht das?**

1. **`--from=builder`**: Kopiert aus Stage 1 (Builder)
2. **`--chown=appuser:appuser`**: Setzt Besitzer
    - Format: `user:group`
    - Wichtig: Files m√ºssen appuser geh√∂ren
3. **`/root/.local`**: Wo pip --user installiert hat
4. **`/home/appuser/.local`**: Ziel im Runtime-Image

**Warum nur .local kopieren?**
- Builder hat viel unn√∂tiges Zeug (Source-Files, Cache)
- Wir brauchen nur compiled Packages
- Spart hunderte MB

---

### 2.5 Code kopieren

```dockerfile
COPY --chown=appuser:appuser ./app ./app
```

- Kopiert `app/` Verzeichnis vom Host ‚Üí `/app/app` im Container
- `--chown`: appuser besitzt die Files

---

### 2.6 PATH & User

```dockerfile
ENV PATH=/home/appuser/.local/bin:$PATH
USER appuser
```

**PATH erweitern:**
- Python-Packages installieren Commands in `.local/bin`
- z.B. `uvicorn` Binary
- PATH muss erweitert werden, sonst findet Shell es nicht

**USER appuser:**
- **Ab hier laufen alle Commands als appuser**
- `CMD` l√§uft als appuser (nicht root)
- Security!

---

### 2.7 Port & CMD

```dockerfile
EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**EXPOSE 8000:**
- **Nur Dokumentation!** √ñffnet Port nicht wirklich
- Sagt: "Dieser Container lauscht auf 8000"
- Kubernetes liest das f√ºr Container-Port

**CMD:**
- Befehl der beim Container-Start l√§uft
- `uvicorn`: ASGI-Server f√ºr FastAPI
- `app.main:app`: Python-Modul (`app/main.py`), Variable `app`
- `--host 0.0.0.0`: **Wichtig!** Lauscht auf allen Netzwerk-Interfaces
    - `127.0.0.1` (localhost) w√ºrde nicht von au√üen erreichbar sein
    - `0.0.0.0` = alle IPs des Containers
- `--port 8000`: Lauscht auf Port 8000

---

## 3. Kubernetes Deployment

### 3.1 Metadata & Labels

```yaml
metadata:
  name: api-gateway
  namespace: video-transcoding
  labels:
    app: api-gateway
    component: backend
    tier: api
```

**Labels - Warum 3?**

Organisationssystem wie Hashtags:

- **`app: api-gateway`**: Hauptidentifikation
    - Service nutzt das f√ºr Selector
    - `kubectl get pods -l app=api-gateway`

- **`component: backend`**: Logische Gruppierung
    - vs. `frontend`, `database`
    - `kubectl get pods -l component=backend`

- **`tier: api`**: Schicht-Modell
    - vs. `tier: data`, `tier: cache`
    - F√ºr Netzwerk-Policies sp√§ter

**Nutzen:**
```bash
# Alle Backend-Pods
kubectl get pods -l component=backend

# Alle API-Tier Pods (k√∂nnte mehrere Services umfassen)
kubectl get pods -l tier=api

# Nur API Gateway
kubectl get pods -l app=api-gateway
```

---

### 3.2 Replicas & Update Strategy

```yaml
replicas: 2

strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 0
```

**Replicas: 2 - Warum?**

1. **High Availability:**
   ```
   Pod 1 stirbt ‚Üí Pod 2 l√§uft weiter ‚Üí Keine Downtime
   ```

2. **Load Balancing:**
   ```
   Request 1 ‚Üí Pod 1
   Request 2 ‚Üí Pod 2
   Request 3 ‚Üí Pod 1
   ```

3. **Rolling Updates ohne Downtime:**
   ```
   Update: Pod 1 ‚Üí v2, Pod 2 noch v1 ‚Üí Traffic geht zu Pod 2
   Dann:   Pod 2 ‚Üí v2, Pod 1 schon v2 ‚Üí Traffic verteilt
   ```

**RollingUpdate Strategy:**

```
Start:    [Pod1 v1] [Pod2 v1]
             ‚Üì
Schritt 1: [Pod1 v1] [Pod2 v1] [Pod3 v2]  ‚Üê maxSurge: 1
             ‚Üì
Schritt 2: [Pod2 v1] [Pod3 v2]             ‚Üê Pod1 wird gel√∂scht
             ‚Üì
Schritt 3: [Pod2 v1] [Pod3 v2] [Pod4 v2]  ‚Üê Neuer Pod
             ‚Üì
Ende:      [Pod3 v2] [Pod4 v2]             ‚Üê Pod2 wird gel√∂scht
```

**Parameter:**
- `maxSurge: 1`: Max 1 Pod **mehr** als `replicas` w√§hrend Update
    - Erlaubt: 3 Pods (2 + 1)
    - Braucht mehr Ressourcen tempor√§r

- `maxUnavailable: 0`: **Minimum** 2 Pods m√ºssen immer laufen
    - **Keine Downtime garantiert**
    - 0 = Alle Pods m√ºssen immer verf√ºgbar sein

**Alternative Strategien:**

- `Recreate`: Alle Pods l√∂schen, dann neue erstellen
    - **Downtime!** Nur f√ºr Dev-Umgebungen

---

### 3.3 Service Account

```yaml
serviceAccountName: api-gateway
```

**Was ist ein ServiceAccount?**

Eine **Identity** f√ºr Pods innerhalb von Kubernetes.

**Analogie:**
```
User Account (f√ºr Menschen)
    ‚Üì kubectl get pods
Kubernetes API
    ‚úÖ Hat Rechte

Service Account (f√ºr Pods)
    ‚Üì Kubernetes Client in Python
Kubernetes API
    ‚úÖ Hat Rechte (wenn RBAC konfiguriert)
```

**Warum brauchen wir das?**

Unser API Gateway muss **Kubernetes Jobs erstellen**:

```python
# In Python sp√§ter:
from kubernetes import client

job = client.V1Job(...)
batch_api.create_namespaced_job(
    namespace="video-transcoding",
    body=job
)
```

**Ohne ServiceAccount:**
```
Pod ‚Üí Kubernetes API: "Create Job"
API: ‚ùå "Forbidden: No permissions"
```

**Mit ServiceAccount + RBAC:**
```
Pod (als api-gateway ServiceAccount)
    ‚Üí Kubernetes API: "Create Job"
    ‚Üí API pr√ºft: ServiceAccount hat Role "job-creator"
    ‚Üí Role erlaubt: Create Jobs in namespace "video-transcoding"
    ‚úÖ Job wird erstellt
```

**RBAC = Role-Based Access Control**

Claude Code hat wahrscheinlich erstellt:
```yaml
# service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: api-gateway
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: job-creator
rules:
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["create", "get", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: api-gateway-job-creator
subjects:
- kind: ServiceAccount
  name: api-gateway
roleRef:
  kind: Role
  name: job-creator
```

---

### 3.4 Environment Variables

```yaml
env:
- name: APP_NAME
  value: "Video Transcoding API Gateway"

- name: KUBERNETES_NAMESPACE
  valueFrom:
    fieldRef:
      fieldPath: metadata.namespace

- name: IN_CLUSTER
  value: "true"
```

**Verschiedene Arten ENV-Vars:**

1. **Hardcoded:**
   ```yaml
   - name: DEBUG
     value: "false"
   ```
    - Einfach, aber nicht flexibel

2. **From Field (Downward API):**
   ```yaml
   - name: KUBERNETES_NAMESPACE
     valueFrom:
       fieldRef:
         fieldPath: metadata.namespace
   ```
    - Pod liest seine eigenen Metadaten
    - `metadata.namespace` ‚Üí Pod wei√ü in welchem Namespace er l√§uft

**Warum wichtig?**

```python
# In Python:
namespace = os.getenv('KUBERNETES_NAMESPACE')
# ‚Üí "video-transcoding"

# Job erstellen im gleichen Namespace:
batch_api.create_namespaced_job(
    namespace=namespace,  # Automatisch korrekt!
    body=job
)
```

**IN_CLUSTER:**
- Sagt Python Kubernetes-Client: "Du l√§ufst in Cluster"
- Nutzt dann ServiceAccount-Token automatisch
- Alternative: `IN_CLUSTER=false` f√ºr lokales Testing

---

### 3.5 Resources

```yaml
resources:
  requests:
    memory: "128Mi"
    cpu: "100m"
  limits:
    memory: "512Mi"
    cpu: "500m"
```

**Requests vs. Limits - Der Unterschied:**

| | Requests | Limits |
|---|----------|--------|
| **Bedeutung** | "Ich brauche mindestens..." | "Ich darf maximal..." |
| **Scheduling** | Pod nur auf Node wenn genug frei | Egal |
| **Garantie** | Ja, bekommt mindestens diese Ressourcen | Nein |
| **√úberschreitung** | Impossible (ist garantiert) | CPU: Throttling, Memory: Kill |

**Beispiel:**

```yaml
requests:
  memory: "128Mi"
  cpu: "100m"
```

**Scheduling:**
```
Node A: 256Mi RAM frei, 500m CPU frei
    ‚Üì Pod braucht 128Mi, 100m
    ‚úÖ Passt, wird gescheduled

Node B: 64Mi RAM frei, 1000m CPU frei
    ‚Üì Pod braucht 128Mi, aber nur 64Mi frei
    ‚ùå Passt nicht, Pod bleibt Pending
```

**Limits:**
```yaml
limits:
  memory: "512Mi"
  cpu: "500m"
```

**CPU Limit:**
- Pod nutzt 600m CPU (mehr als 500m Limit)
- ‚Üí Kubernetes **drosselt** CPU auf 500m
- Pod l√§uft langsamer, aber stirbt nicht

**Memory Limit:**
- Pod nutzt 600Mi Memory (mehr als 512Mi Limit)
- ‚Üí Kubernetes **killt** Pod
- Reason: `OOMKilled` (Out Of Memory)
- Pod wird neu gestartet

**Einheiten:**
- `128Mi` = 128 Mebibyte (2^20 bytes) ‚âà 134 MB
- `100m` = 100 millicores = 0.1 CPU cores
- `1000m` = 1 CPU core

**Warum Request < Limit?**

```
Normal-Last: 128Mi ‚Üí innerhalb Request, garantiert
Burst-Last:  400Mi ‚Üí √ºber Request, aber unter Limit, erlaubt
Leak/Bug:    600Mi ‚Üí √ºber Limit ‚Üí Pod wird killed
```

**Best Practice:**
- Request: Was du **normalerweise** brauchst
- Limit: Was du **maximal** tolerierst (Sicherheitsnetz)

---

### 3.6 Liveness Probe

```yaml
livenessProbe:
  httpGet:
    path: /api/v1/health
    port: http
  initialDelaySeconds: 10
  periodSeconds: 30
  timeoutSeconds: 5
  failureThreshold: 3
```

**Was ist Liveness?**

**Frage:** "Lebst du noch?"

**Zweck:** Container neu starten wenn er "h√§ngt".

**Ablauf:**

```
Container startet
    ‚Üì (warte 10 Sekunden - initialDelaySeconds)
kubelet: GET /api/v1/health
    ‚Üì (200 OK in < 5 Sekunden?)
    ‚úÖ Healthy
    ‚Üì (warte 30 Sekunden - periodSeconds)
kubelet: GET /api/v1/health
    ‚Üì (500 Error oder Timeout?)
    ‚ö†Ô∏è Failure 1/3
    ‚Üì (warte 30 Sekunden)
kubelet: GET /api/v1/health
    ‚Üì (Timeout?)
    ‚ö†Ô∏è Failure 2/3
    ‚Üì (warte 30 Sekunden)
kubelet: GET /api/v1/health
    ‚Üì (Timeout?)
    ‚ùå Failure 3/3 ‚Üí RESTART CONTAINER
```

**Parameter:**

- `initialDelaySeconds: 10`: Warte 10 Sek nach Start
    - App braucht Zeit zum Booten
    - Zu kurz ‚Üí False-Positive Restarts

- `periodSeconds: 30`: Pr√ºfe alle 30 Sekunden
    - Nicht zu oft (unn√∂tige Last)
    - Nicht zu selten (langsame Erkennung)

- `timeoutSeconds: 5`: Max 5 Sek f√ºr Response
    - L√§nger ‚Üí gilt als Failure

- `failureThreshold: 3`: 3 Failures ‚Üí Restart
    - Verhindert Restart bei kurzen Hickups

**Use Cases:**

‚úÖ **Deadlock:** Thread h√§ngt, Server reagiert nicht ‚Üí Restart
‚úÖ **Memory Leak:** App wird langsam, /health timeout ‚Üí Restart
‚úÖ **Database Connection Lost:** App funktioniert nicht ‚Üí Restart

---

### 3.7 Readiness Probe

```yaml
readinessProbe:
  httpGet:
    path: /api/v1/ready
    port: http
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
```

**Was ist Readiness?**

**Frage:** "Bist du bereit f√ºr Traffic?"

**Unterschied zu Liveness:**

| | Liveness | Readiness |
|---|----------|-----------|
| **Frage** | Lebst du? | Bereit f√ºr Traffic? |
| **Bei Failure** | Container restart | Aus Service entfernen |
| **Zweck** | H√§ngende Container fixen | Kein Traffic zu nicht-bereiten Pods |

**Use Case:**

```
Pod startet
    ‚Üì L√§dt gro√üe Config-Datei (10 Sekunden)
    ‚Üì Readiness: Not Ready
    ‚Üì Service: Sendet KEINEN Traffic zu diesem Pod
    ‚Üì
Config geladen
    ‚Üì Readiness: Ready
    ‚Üì Service: Sendet Traffic zu diesem Pod
```

**Warum beide Probes?**

```
Pod hat tempor√§res Problem (DB-Connection lost)
    ‚Üì Readiness fails ‚Üí Kein Traffic
    ‚Üì (wartet auf DB-Reconnect)
    ‚Üì Readiness succeeds ‚Üí Traffic wieder
    ‚Üì Liveness war die ganze Zeit OK ‚Üí Kein Restart
```

**Beispiel Rolling Update:**

```
Update: Neuer Pod startet
    ‚Üì Readiness: Not Ready (startet noch)
    ‚Üì Service: Sendet Traffic zu alten Pods
    ‚Üì
Neuer Pod: Ready
    ‚Üì Service: Sendet Traffic zu neuem + alten Pods
    ‚Üì
Alter Pod: Wird gel√∂scht
    ‚Üì Service: Sendet nur noch Traffic zu neuem Pod
```

**Zero-Downtime!**

---

### 3.8 Volumes

```yaml
volumeMounts:
- name: uploads
  mountPath: /tmp/uploads
- name: outputs
  mountPath: /tmp/outputs

volumes:
- name: uploads
  emptyDir:
    sizeLimit: 10Gi
- name: outputs
  emptyDir:
    sizeLimit: 10Gi
```

**Was ist emptyDir?**

- **Tempor√§res** Verzeichnis
- Erstellt wenn Pod startet
- **Gel√∂scht** wenn Pod stirbt
- Geteilt zwischen Containern im Pod

**Lifecycle:**

```
Pod startet
    ‚Üí emptyDir erstellt (leer)
    ‚Üí Container schreibt Dateien
    ‚Üí Dateien bleiben w√§hrend Pod l√§uft
    ‚Üí Pod stirbt
    ‚Üí emptyDir wird GEL√ñSCHT
```

**Warum emptyDir f√ºr Videos?**

‚ùå **NICHT f√ºr finale Videos!**
- Pod stirbt ‚Üí Videos weg
- Nur f√ºr **tempor√§re** Verarbeitung

**Flow:**

```
1. Video-Upload ‚Üí /tmp/uploads/video.mp4 (emptyDir)
2. Transcoding ‚Üí liest von /tmp/uploads
3. Output ‚Üí schreibt nach /tmp/outputs/video-720p.mp4 (emptyDir)
4. Upload to S3 ‚Üí von /tmp/outputs
5. Delete aus emptyDir ‚Üí Speicher frei
```

**sizeLimit: 10Gi:**
- Max 10 Gigabyte
- Verhindert dass Pod ganzen Node-Speicher f√ºllt
- √úber Limit ‚Üí Pod wird evicted (aus Node geworfen)

**Sp√§ter:**
- **PersistentVolume** f√ºr dauerhafte Speicherung
- **S3/MinIO** f√ºr Object Storage
- emptyDir nur f√ºr Temp-Files

---

## 4. Kubernetes Service

```yaml
type: ClusterIP

selector:
  app: api-gateway

ports:
- name: http
  protocol: TCP
  port: 80
  targetPort: http
```

**Was macht ein Service?**

**Problem ohne Service:**

```
Client will API aufrufen
    ‚Üí Welche Pod-IP?
    ‚Üí Pod 1: 10.244.1.5
    ‚Üí Pod 2: 10.244.2.8
    ‚Üí Pod stirbt ‚Üí neue IP!
```

**L√∂sung mit Service:**

```
Client ‚Üí Service (10.96.87.61 - stabile IP)
    ‚Üí Load Balancer
    ‚Üí Pod 1 (10.244.1.5)
    ‚Üí Pod 2 (10.244.2.8)
```

**Service DNS:**
```
api-gateway.video-transcoding.svc.cluster.local
```

Von anderem Pod im Cluster:
```bash
curl http://api-gateway.video-transcoding.svc.cluster.local
# oder kurz:
curl http://api-gateway
```

**ClusterIP:**
- Nur innerhalb Cluster erreichbar
- **NICHT** von au√üen (deinem Browser)

**F√ºr External Access (sp√§ter):**
- **Ingress** (HTTP/S Routing)
- **NodePort** (Port auf allen Nodes)
- **LoadBalancer** (Cloud Load Balancer)

**Ports:**

```yaml
port: 80          # Service lauscht auf Port 80
targetPort: http  # Leitet zu Container-Port "http" (8000)
```

**Flow:**

```
Request ‚Üí Service:80 ‚Üí Load Balancer ‚Üí Pod:8000
```

**targetPort: http** referenziert:
```yaml
# In Deployment:
ports:
- name: http      # ‚Üê Dieser Name
  containerPort: 8000
```

---

## 5. Was haben wir erreicht?

### Code

‚úÖ **FastAPI App** mit:
- Root Endpoint (/)
- Health Endpoints (/api/v1/health, /api/v1/ready)
- CORS f√ºr Frontend-Zugriff
- Startup/Shutdown Events
- Settings-Management

### Docker

‚úÖ **Multi-Stage Dockerfile** mit:
- Builder Stage (Dependencies installieren)
- Runtime Stage (nur Runtime-Binaries)
- Non-Root User (Security)
- Optimierte Layer (schnelles Rebuild)

### Kubernetes

‚úÖ **Deployment** mit:
- 2 Replicas (High Availability)
- Rolling Updates (Zero Downtime)
- Health Probes (Self-Healing)
- Resource Limits (Cluster Protection)
- ServiceAccount (API-Zugriff)

‚úÖ **Service** mit:
- Load Balancing
- Stabile IP/DNS
- Port-Mapping

---

## 6. Was fehlt noch?

### Funktionalit√§t

‚ùå Upload Endpoint (`POST /api/v1/upload`)
‚ùå Job Controller (Kubernetes Job erstellen)
‚ùå Status Endpoint (`GET /api/v1/jobs/{id}`)
‚ùå Download Endpoint (`GET /api/v1/download/{id}`)

### Testing

‚ùå Docker Image bauen
‚ùå Image in Kind laden
‚ùå Kubernetes Manifests anwenden
‚ùå Functionality Tests

### Storage

‚ùå Persistent Storage (PV/PVC)
‚ùå Object Storage (MinIO/S3)
‚ùå Database (PostgreSQL f√ºr Metadata)

---

## 7. N√§chste Schritte

### Phase 1: Deployment (Jetzt)

1. Docker Image bauen
2. Image in Kind-Cluster laden
3. ServiceAccount + RBAC erstellen
4. Deployment anwenden
5. Service anwenden
6. Port-Forward testen
7. Health Endpoints testen

### Phase 2: Upload Feature

1. Upload Endpoint implementieren
2. File Validation (Format, Size)
3. Temporary Storage (emptyDir)
4. Testing

### Phase 3: Job Controller

1. Kubernetes Job Template
2. Job-Erstellung aus API
3. Job Status Monitoring
4. Cleanup

### Phase 4: Transcoding Worker

1. FFmpeg Container
2. Job Implementation
3. Input/Output Handling
4. Testing

---

## 8. Learnings & Best Practices

### FastAPI

‚úÖ Async/Await f√ºr I/O-intensive Tasks
‚úÖ Auto-Documentation spart Entwicklungszeit
‚úÖ CORS Middleware f√ºr Frontend-Integration
‚úÖ Startup Events f√ºr Initialisierung

### Docker

‚úÖ Multi-Stage Builds = kleinere Images
‚úÖ Non-Root User = Security
‚úÖ Layer-Optimierung = schnelleres Rebuild
‚úÖ `--no-cache-dir` = weniger Speicher

### Kubernetes

‚úÖ 2+ Replicas = High Availability
‚úÖ RollingUpdate + Probes = Zero Downtime
‚úÖ Resource Limits = Cluster Protection
‚úÖ ServiceAccount = Least Privilege
‚úÖ Labels = Flexibles Organisieren

### Architecture

‚úÖ API Versioning (/api/v1) = Abw√§rtskompatibilit√§t
‚úÖ Health Endpoints = Monitoring & Orchestration
‚úÖ Settings-Management = Umgebungs-Flexibilit√§t
‚úÖ Separation of Concerns = Wartbarkeit

---

**Erstellt:** 08.02.2026
**Status:** Code Review abgeschlossen, bereit f√ºr Deployment-Test